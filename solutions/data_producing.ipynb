{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import glob\n",
    "from IPython.display import Audio\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from Freesound.data import *\n",
    "from Freesound.utils import *\n",
    "from Freesound.model import *\n",
    "from Freesound.augmentations import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed_everything(0)\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"logs/logs.log\", filemode=\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/src/workspace/data/files/\"\n",
    "train_dataset_dir = os.path.join(dataset_dir, \"Training_Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sorted(glob.glob(os.path.join(train_dataset_dir, '**/*.wav'), recursive=True))\n",
    "y = np.array([1 if \"human\" in i else 0 for i in X])\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    #RandomParameter(VTLP, [[0.8, 1.2]], p=0.5),\n",
    "    #MinMaxChunkScaler(),\n",
    "    #Normalize(),\n",
    "    #RandomParameter(RandomNoise, [[0.01, 0.1]]),\n",
    "    #RandomParameter(Shift, [[2000, 32000]]),\n",
    "    #RandomParameter(TimeStretch, [[0.75, 1.3]]),\n",
    "    #RandomParameter(PitchShift, [[-8, 8]]),\n",
    "    #RandomParameter(Distortion, [[-1, -0.3], [.3, 1.]]),\n",
    "    ToMellSpec(n_mels=128),\n",
    "    #GetMFCC(),\n",
    "    #PadOrClip(300),\n",
    "    #Normalize(),\n",
    "    #ToTensor(),\n",
    "    #transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_Train(X_train, y_train, train_transform)\n",
    "val_dataset = Dataset_Train(X_val, y_val, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_and_save_one_epoch(folder, run, dataset):\n",
    "    for sample in tqdm(range(len(dataset))):\n",
    "        k = dataset[sample][0]\n",
    "        name = f\"{run}_{sample}\"\n",
    "        path = os.path.join(folder, name)\n",
    "        np.save(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(sample, dataset, run, folder):\n",
    "    k = dataset[sample][0]\n",
    "    name = f\"{run}_{sample}\"\n",
    "    path = os.path.join(folder, name)\n",
    "    np.save(path, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_and_save_one_epoch_parallel(run, folder, dataset):\n",
    "    before = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        for sample in range(len(dataset)):\n",
    "            future = executor.submit(save_sample, sample, dataset, run, folder)\n",
    "    \n",
    "    print(time.time() - before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precalculate_and_save_one_epoch_joblib(run, folder, dataset):\n",
    "    Parallel(n_jobs=16, verbose=5, backend=\"multiprocessing\")(delayed(save_sample)(sample, dataset, run, folder)\n",
    "                        for sample in range(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ../data/files/raw_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data/files/raw_mels\n",
    "!mkdir ../data/files/raw_mels_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend MultiprocessingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  80 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=16)]: Done 620 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=16)]: Done 1376 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=16)]: Done 2348 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=16)]: Done 3536 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=16)]: Done 4940 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=16)]: Done 6560 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=16)]: Done 8396 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=16)]: Done 10448 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=16)]: Done 12716 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=16)]: Done 15200 tasks      | elapsed:   57.2s\n",
      "[Parallel(n_jobs=16)]: Done 17900 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=16)]: Done 20816 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=16)]: Done 23948 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=16)]: Done 27296 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=16)]: Done 30860 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=16)]: Done 34640 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=16)]: Done 38636 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=16)]: Done 40000 out of 40000 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=16)]: Using backend MultiprocessingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  48 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 496 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=16)]: Done 1252 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=16)]: Done 2224 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=16)]: Done 3412 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=16)]: Done 4816 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=16)]: Done 6436 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=16)]: Done 8272 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=16)]: Done 10000 out of 10000 | elapsed:   22.1s finished\n"
     ]
    }
   ],
   "source": [
    "for num in range(0, 1):\n",
    "    precalculate_and_save_one_epoch_joblib(num, \"../data/files/raw_mels\", train_dataset)\n",
    "    precalculate_and_save_one_epoch_joblib(num, \"../data/files/raw_mels_val\", val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
